{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS207 Project Group 9\n",
    "# Milestone 1\n",
    "\n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The software implements **‘Automatic Differentiation’ (AD)**. This is a technique to computationally evaluate the derivative of a specified function. Importantly, AD is not the same as symbolic differentiation or numerical differentiation, and holds important advantages over both. Symbolic differentiation, which is equivalent to analytically solving differential equations, can find the exact solution (with machine precision), but is very computationally expensive, and so with very large functions can be infeasible. Numerical differentiation, which uses the finite-difference approximation method, is computationally efficient, but is ultimately only approximate, and can be subject to both rounding error and discretisation error, meaning that it cannot be perfectly accurate. Both of these ‘traditional’ methods of differentiation run into problems and inefficiencies when calculating higher derivatives and partial derivatives with respect to many inputs (which is an important component of gradient-based optimisation methods). \n",
    "\n",
    "\n",
    "Automatic differentiation solves all these problems as it is able to solve derivatives to machine precision with comparative computational efficiency. As a result, automatic differentiation has incredibly important applications; in its ‘reverse-mode’ (discussed below), it is the basis of back-propagation, a fundamental process in neural network machine learning algorithms - as such this technique is leveraged by open-source machine learning libraries such as TensorFlow. A result of its efficient accuracy and iterative method, AD is capable of algorithmic differentiation: Because of the fact that every computer program, from mathematical algorithms to web-pages, can be expressed as a sequence and combination of arithmetic operations and elementary functions, the derivative of any computer program can be found using automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic differentiation is essentially the iterative application of the chain-rule. As mentioned above, any function can be considered a sequence of basic arithmetic operations or elementary functions (addition, multiplication, division, subtraction, trigonometric functions, exponential, logarithms etc.) and so any function can be interpreted in the following way (albeit often less simply):\n",
    "\t$$y = f(x) = f(g(h((x)))$$\n",
    "\n",
    "This can be rewritten as:\n",
    "$$y = f(g(h(x0))) = f(g(x1)) = f(x2) = x3$$\n",
    "\t\n",
    "Often, this decomposition is represented as an acyclic, directed computational graph that illustrates the route from the base function x0 to y, as illustrated by the example below:\n",
    "\n",
    "$ x_0\\rightarrow^{h(x)}x_1\\rightarrow^{g(x)}x_2\\rightarrow^{f(x)}x_3\\rightarrow y $\n",
    "\n",
    "\n",
    "\n",
    "In forward mode, automatic differentiation works by decomposing the function into this structure, and working through each component function finding the derivative using the chain rule ‘inside out’. That is to say, dx0/dx is found first, following by dx1/dx and so on until dy/dx itself is found. All this requires initial values to be set for x0, and x0’.\n",
    "\n",
    "\n",
    "Reverse mode, however, works in the opposite direction; rather than finding the derivative of the most fundamental component, and then finding the derivative of parent expressions in terms of these children components recursively until the final gradient is found, reverse mode goes the other way. It finds the derivative of each ‘child’ function in terms of its parent function recursively until the basic level derivative is found, at which point the final gradient can be found.\n",
    "\n",
    "\n",
    "One way of achieving forward mode AD is to use dual numbers. These are an extension of real numbers, somewhat analogous to imaginary numbers, such that every number additionally contains a dual component, $\\epsilon$, where $\\epsilon^2$ = 0. Given any polynomial function (or, in fact, any analytic real function via its Taylor series), if we replace x with (x+x'$\\epsilon$), we find that the function will become: f(x) + f'(x)$\\epsilon$. This provides a routine to automatically compute the derivative of the function f(x), and so is used in forward AD.\n",
    "\n",
    "Sources: https://en.wikipedia.org/wiki/Automatic_differentiation,\n",
    "\t   http://www.columbia.edu/~ahd2125/post/2015/12/5/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. How to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _a) Installation_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the best way to install the package is to download or clone the package's Github repo (https://github.com/CS207-Project-Group-9/cs207-FinalProject). Then, one must ensure that the package's requirements (Numpy and numbers) are installed, or install them manually. The user can then create a new driver python script\n",
    "\n",
    "The package will soon be available on PyPI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i) Importing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package can be imported simply through:\n",
    "```python\n",
    "from AutoDiff import AutoDiff\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following code only to allow use in docs folder:\n",
    "import os\n",
    "path = os.getcwd().replace('docs','')\n",
    "os.chdir(path)\n",
    "\n",
    "# following code for normal import:\n",
    "from AutoDiff import AutoDiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) Forward Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Univariate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a simple, univariate example, let's find the value of the derivate of $y=3x^2-4x$  at $x=3$.\n",
    "\n",
    "we create an instance of the AutoDiff object as the basic building block for the equation - in other words, a single value of the independent variable. This object can then be used with binary and unary mathematical operators to construct the full function being evaluated. For each operation, a new function value (AutoDiff.val) and derivative value (AutoDiff.der) are calculated, such that once all operations are complete for the function, the function object's 'der' attribute will be that function's derivative at the point specificied at AutoDiff object creation.\n",
    "\n",
    "**N.B.**: It is important to note that AutoDiff assumes that the object being initialised is elementary and as such will have a derivative value of 1. Users can pass different der values as an argument for the function if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14.]]\n",
      "15.0 14.0\n"
     ]
    }
   ],
   "source": [
    "a = 3.0\n",
    "x = AutoDiff.fAD(a)\n",
    "y = 3*x**2 - 4*x\n",
    "\n",
    "print(y.der)\n",
    "\n",
    "# alternative way to find values:\n",
    "\n",
    "print(y.get_val(), y.get_jac())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complex cases can be handled, for example:\n",
    "\n",
    "$$y = \\frac{e^{2x}|1-\\log{x}|}{\\sin{x} - \\cos{x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215.62712855401998"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = AutoDiff.fAD(a)\n",
    "\n",
    "y = (AutoDiff.exp(2*x)*abs(1-AutoDiff.log(x)))/(AutoDiff.sin(x) - AutoDiff.cos(x))\n",
    "\n",
    "y.get_jac()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multivariate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoDiff.fAD is designed to handle multivariate calculus as well. For this, the additional functions **create_f** and **stack_f** must be used.\n",
    "\n",
    "Create_f is used to set up a system of fAD objects, in the form of a list, for construction of a multivariable function, whilst stack_f can be used to take in multiple independent fAD objects and construct a new fAD object that combines these objects such that the values of all objects can be returned at once and the objects can be altered simultaneously.\n",
    "\n",
    "**AutoDiff.create_f - for creating multiple variables for a single function**\n",
    "\n",
    "Multiple variables can be passed to create_f **as a list** and it will return an object for each variable with the necessary set of partial derivative values. When finding the derivative of a function with more than one variable, these variables should _always_ be created together using create_f, as this will ensure each variable has the correct vector of partial derivative values. Moreover, variables from unrelated functions should not be created together with create. Passing both $a$ and $b$ into create_f will create partial derivatives of $a$ with respect to $b$ and $b$ with respect to $a$ - if this is not desired then $a$ and $b$ should be created separately. Note that just like the single-variable case, create_f will assume that each object being created is elementary and will assign a derivative value of 1 to each unless otherwise instructed by the user.\n",
    "\n",
    "**AutoDiff.stack_f - for creating a vector of independent functions**\n",
    "\n",
    "One can pass multiple fAD objects into stack_f **as a list** and it will return a single fAD object with each of the objects values and derivatives stored as a vector. This allows one to return the derivative of all objects at once, or to manipulate each function in the same way all at once. Calling the `val` attribute of the stacked AutoDiff object will return the value of each function at the specified point. Calling the `der` attribute will return the Jacobian of the derivatives.\n",
    "\n",
    "**N.B.** Creating a stack_f object for vector value functions allows each function within the vector to be altered uniformly and simultaneously (see below) - but only operations with scalars (i.e. not other fAD objects) are currently supported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's deal with the following example of a vector-valued function:\n",
    "\n",
    "$\\begin{bmatrix} f_1 \\\\ f_2 \\\\ f_3 \\end{bmatrix} = \\begin{bmatrix} x^2+2y^2 \\\\ |x-\\cos{y}| \\\\ x-\\frac{y}{x-y} \\end{bmatrix}$\n",
    "\n",
    "at the points:\n",
    "\n",
    "$x = 4$, $y = 6$\n",
    "\n",
    "Once we've found the values and derivatives for this vector valued function $F(x,y)$, let's find the values and derivatives of another vector valued function, $G$:\n",
    "\n",
    "$G(x,y) = 2F(x,y)+5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F function values:  [88.          3.03982971  7.        ]\n",
      "F derivative values:\n",
      " [[ 8.        24.       ]\n",
      " [ 1.        -0.2794155]\n",
      " [ 2.5       -1.       ]]\n",
      "---------------------------\n",
      "G Function values:  [181.          11.07965943  19.        ]\n",
      "G Derivative values:\n",
      " [[16.       48.      ]\n",
      " [ 2.       -0.558831]\n",
      " [ 5.       -2.      ]]\n"
     ]
    }
   ],
   "source": [
    "x, y = AutoDiff.create_f([4,6])\n",
    "\n",
    "f1 = x**2 + 2*y**2\n",
    "f2 = abs(x - AutoDiff.cos(y))\n",
    "f3 = x - y/(x-y)\n",
    "\n",
    "F = AutoDiff.stack_f([f1,f2,f3])\n",
    "\n",
    "print('F function values: ', F.get_val())\n",
    "print('F derivative values:\\n', F.get_jac())\n",
    "\n",
    "G = F*2+5\n",
    "print(\"---------------------------\")\n",
    "print('G Function values: ', G.get_val())\n",
    "print('G Derivative values:\\n', G.get_jac())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iii) Usage: Reverse Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoDiff can also implement reverse mode automatic differentiation through the rAD class. The usage is slightly different. Variables can be initialised in a similar way, however this time, multiple variables within one equation do not necessarily need to be defined at once using create_r (although this function still exists should one want to use it). Once the function has been defined, the user must explicitly state which variable is the 'outer' variable of the function using the method outer(). then, to find the gradient of the outer variable with respect to, for example, the variable x, one must call **x**.grad().\n",
    "\n",
    "This is because, unlike forward mode which determines values and derivatives in the same directions whilst implicitly traversing the function's computational graph, reverse mode traverses the computational graph in one direction, storing the connections between nodes as it does so, to determine function values, and then to find function derivatives it must traverse the computational graph in the reverse direction. This is done by recursing through the from x (inner object) to y (outer object) and then calculating the derivative through the recursion.\n",
    "\n",
    "Let's again find the value of the derivate of $y=3x^2-4x$  at $x=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 3\n",
    "\n",
    "x = AutoDiff.rAD(a)\n",
    "\n",
    "y = 3*x**2-4*x\n",
    "y.outer()\n",
    "\n",
    "x.grad()\n",
    "\n",
    "x.get_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complex cases can be handled, for example:\n",
    "\n",
    "$$y = \\frac{e^{2x}|1-\\log{x}|}{\\sin{x} - \\cos{x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215.62712855402003"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = AutoDiff.rAD(a)\n",
    "\n",
    "y = AutoDiff.exp(2*x)*abs(1-AutoDiff.log(x))/(AutoDiff.sin(x) - AutoDiff.cos(x))\n",
    "\n",
    "y.outer()\n",
    "x.grad()\n",
    "\n",
    "x.get_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Multivariate_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate cases can also be handled. Again, let's find the derivative of:\n",
    "\n",
    "$\\begin{bmatrix} f_1 \\\\ f_2 \\\\ f_3 \\end{bmatrix} = \\begin{bmatrix} x^2+2y^2 \\\\ |x-\\cos{y}| \\\\ x-\\frac{y}{x-y} \\end{bmatrix}$\n",
    "\n",
    "at the points:\n",
    "\n",
    "$x = 4$, $y = 6$\n",
    "\n",
    "This time round, things are a bit more complicated, as gradients must be reset between the functions; the variables can only be defined in the context of one outer function at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0 24.0\n",
      "1.0 -0.27941549819892586\n",
      "2.5 -1.0\n"
     ]
    }
   ],
   "source": [
    "x, y = AutoDiff.create_f([4,6])\n",
    "\n",
    "x = AutoDiff.rAD(4)\n",
    "y = AutoDiff.rAD(6)\n",
    "\n",
    "f1 = x**2 + 2*y**2\n",
    "f1.outer()\n",
    "x.grad()\n",
    "y.grad()\n",
    "print(x.get_grad(),y.get_grad())\n",
    "AutoDiff.reset_der((x,y))\n",
    "\n",
    "f2 = abs(x - AutoDiff.cos(y))\n",
    "f2.outer()\n",
    "x.grad()\n",
    "y.grad()\n",
    "print(x.get_grad(),y.get_grad())\n",
    "AutoDiff.reset_der((x,y))\n",
    "\n",
    "f3 = x - y/(x-y)\n",
    "f3.outer()\n",
    "x.grad()\n",
    "y.grad()\n",
    "print(x.get_grad(),y.get_grad())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Software Organisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _a) Directory Structure_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cs207-FinalProject\\\n",
    "                   AutoDiff/                   \n",
    "                            __init__.py         \n",
    "                            __pycache__.py\n",
    "                            AutoDiff.py\n",
    "                            test_AutoDiff.py\n",
    "                   docs/\n",
    "                        milestone1.ipynb\n",
    "                        milestone2.ipynb\n",
    "                        final.ipynb\n",
    "                   README.md\n",
    "                   requirement.txt\n",
    "                   setup.cfg\n",
    "```                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _b) Modules_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All code is contained within the **AutoDiff** module. Within here are two Python modules:\n",
    "\n",
    "1. AutoDiff.py: This contains all functional code for automatic differentiation. within this file there are two classes, each with two complementary functions for equation construction along with numerous mathematical functions:\n",
    "    * fAD - Class object that is the basic building block for forward-mode differentiation. Stores function and derivative values and has overloaded mathematical methods to allow construction of mathematical functions.\n",
    "    * create_f - Function that is used to create multiple fAD objects that are within the same function and thus contain partial derivatives with respect to each other.\n",
    "    * stack_f - stacks multiple fAD objects, given as a list, into a single fAD object that represents a vector-valued function.\n",
    "    * rAD - Class object that is the basic building block for reverse-mode differentiation. Stores function value and children lists that, when the grad() method is called, are defined iteratively through reverse mode. Also contains a function outer() which is required to define the final parent node of the function's computational graph.\n",
    "    * create_r - analogous function to its forward-mode counterpart, though not strictly necessary for multivariate functions as create_f is for forward mode.\n",
    "    * stack_r - analogous to forward mode counterpart, creates rAD object for multiple functions - unlike forward mode, the functions (as python functions or lambda functions) must be passed to stack alongside the rAD objects. returns jacobian matrix of derivatives.\n",
    "    * mathematical functions - designed to handle forward and reverse mode objects as well as standard numerics: includes sin, cos, tan, arcsin, arccos, arctan, sinh, cosh, tanh, exp, log, and sqrt.\n",
    "    * mul_by_row - Allows multiplication of forward-mode autodiff object with 2-dimensional derivatives, used for generalised overloading of multiplicative magic methods.\n",
    "    * reset_ders - reset derivative values of reverse mode objects so that they can be reused in new functions.\n",
    "2. test_AD.py: This is the test-suite for AutoDiff.py. Tests for correct function of above classes and functions. Tests are run using pytest, with the assistance of numpy.testing functions `assert_array_equal` and `assert_array_almost_equal` for dealing with cases where values are subject to a degree of rounding error. The tests are linked with Travis CI, which provides continuous integration software testing, and Coveralls, which provides a code coverage service to ensure that all of the code is being tested.\n",
    "\n",
    "\n",
    "Currently, the way to install this package is to download or clone it from the package's repo on Github (https://github.com/CS207-Project-Group-9/cs207-FinalProject), manually install requirements if not present, and create driver script in project directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _c) Implementation_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class: fAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class encapsulates the fundamental machinery of forward mode automatic differentiation, and is capable of dealing with both single and multi-variable cases.\n",
    "\n",
    "#### Dependencies\n",
    "\n",
    "- numpy (imported as np): used for numerous mathematical (e.g. trigonometric, logarithmic operations)\n",
    "- numbers: used to ensure user passes numerical values.\n",
    "\n",
    "#### Attributes (Data Structures)\n",
    "- val: array of floats (of size 1 or more)\n",
    "    - Numeric values, indicating the value of each entry in the current AutoDiff instance. For cases with only one function, val will size 1. For vector-valued functions, val can be longer.\n",
    "    \n",
    "- der: 2D array of floats\n",
    "    - Values representing the derivative value(s) in the current AutoDiff instance. The returned 2D array can be thought of as the Jacobian of all functions and variables for the AutoDiff instance. Suppose there are m elementary variables constructing n functions, all stored within the AutoDiff instance, 'der' would be a n\\*m array of elements, with the (i,j) entry representing the derivative value of the i-th AutoDiff with respect to the j-th elementary variable.\n",
    "\n",
    "#### Methods \n",
    "(The following demonstrations are for the case when there is only one value in self.val. When the fAD object is in higher dimension, storing the values in an array allows us to simply apply the computation to each entry.)\n",
    "\n",
    "0. `__init__`:\n",
    "    - arguments: \n",
    "        - a list/array of fAD instances or numerics\n",
    "    - sets self.val as a list of 'val' attributes of the input AD instances\n",
    "    - combines the 'der' attributes of the input fAD instances as a 2D array and save as self.der\n",
    "\n",
    "\n",
    "1. `__add__` & `__radd__`: \n",
    "    - arguments:\n",
    "        - self\n",
    "        - other: a float, int, or fAD\n",
    "    - returns: \n",
    "        - if other is an AD -> a new fAD instance with new.val = self.val + other.val, new.der = self.der + other.der\n",
    "        - if other is a numeric value -> a new fAD instance with new.val = self.val + other, new.der = self.der\n",
    "\n",
    "\n",
    "2. `__sub__`\n",
    "    - arguments:\n",
    "\t\t- self\n",
    "\t\t- other: a float, int, or fAD\n",
    "\t- returns: \n",
    "\t\t- if other is a fAD -> a new fAD instance with new.val = self.val - other.val, new.der = self.der - other.der\n",
    "\t\t- if other is a numeric value -> a new fAD instance with new.val = self.val - other, new.der = self.der\n",
    "        \n",
    "        \n",
    "3. `__rsub__`\n",
    "    - arguments:\n",
    "\t\t- self\n",
    "\t\t- other: a float, int, or fAD\n",
    "\t- returns: \n",
    "\t\t- if other is a fAD -> a new fAD instance with new.val = other.val - self.val, new.der = other.der - self.der\n",
    "\t\t- if other is a numeric value -> a new fAD instance with new.val = other - self.val, new.der = -self.der\n",
    "\n",
    "\n",
    "4. `__mul__` & `__rmul__`\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t\t- other: a float, int, or fAD\n",
    "\t- returns: \n",
    "\t\t- if other is a fAD -> a new fAD instance with new.val = self.val \\* other.val, new.der = self.val \\* other.der + self.der \\* other.val\n",
    "\t\t- if other is a numeric value -> a new fAD instance with new.val = self.val \\* other, new.der = self.der \\* other\n",
    "\n",
    "\n",
    "5. `__truediv__`\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t\t- other: a float, int, or fAD\n",
    "\t- returns: \n",
    "\t\t- if other is a fAD -> a new fAD instance with new.val = self.val / other.val, new.der = self.der\\/other.val-self.val\\*other.der\\/(other.val\\*\\*2)\n",
    "\t\t- if other is a numeric value -> a new fAD instance with new.val = self.val / other, new.der = self.der / other\n",
    "\t- raises:\n",
    "\t\t- ZeroDivisionError when other.val = 0 or other = 0\n",
    "\n",
    "\n",
    "6. `__rtruediv__`\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t\t- other: a float, int, or fAD\n",
    "\t- returns: \n",
    "\t\t- if other is a fAD -> a new fAD instance with new.val = other.val / self.val, new.der = other.val / self.der-other.val\\*self.der / (self.val\\*\\*2)\n",
    "\t\t- if other is a numeric value -> a new fAD instance with new.der = -other \\* self.der / (self.val \\*\\*2)\n",
    "\t- raises:\n",
    "\t\t- ZeroDivisionError when self.val = 0\n",
    "\n",
    "\n",
    "7. `__pow__`\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t\t- exp: a float, int, or fAD\n",
    "\t- returns:\n",
    "\t\t- if exp is a fAD -> a new fAD instance with new.val = self.val \\*\\* exp.val, new.der = (self.val\\*\\*exp.val) * (self.der\\*exp.val /self.val + exp.der\\*np.log(self.val))\n",
    "\t\t- if other is a numeric value -> a new fAD instance with new.val = self.val \\*\\* exp, new.der = exp*(self.val\\*\\*(exp-1))\\*self.der\n",
    "\n",
    "\n",
    "8. `__rpow__`\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t\t- base: a float, int, or fAD\n",
    "\t- returns:\n",
    "\t\t- if base is a fAD -> a new fAD instance with new.val = base.val\\*\\*self.val, new.der = (base.val\\*\\*self.val) * (base.der\\*self.val /base.val + self.der\\*np.log(base.val))\n",
    "\t\t- if base is a numeric value -> a new fAD instance with new.val = base\\*\\*self.val, new.der = np.log(base)\\*(base\\*\\*self.val)\\*self.der\n",
    "\n",
    "\n",
    "9. `__neg__`\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t- returns:\n",
    "\t\t- a new fAD instance with new.val = -self.val, new.der = -self.der\n",
    "        \n",
    "\n",
    "10. `__abs__`\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t- returns:\n",
    "\t\t- a new fAD instance with new.val = abs(self.val), new.der = (self.val / abs(self.val)) \\* self.der\n",
    "\n",
    "\n",
    "\n",
    "11. `__eq__`\n",
    "    - arguments:\n",
    "        - self\n",
    "        - other: a fAD instance\n",
    "    - returns:\n",
    "        - 'True' if self.val==other.val and self.der==other.der, 'False' otherwise\n",
    "        \n",
    "12. `__ne__`\n",
    "    - arguments:\n",
    "        - self\n",
    "        - other: a fAD instance\n",
    "    - returns:\n",
    "        - 'False' if self.val==other.val and self.der==other.der, 'True' otherwise\n",
    "        \n",
    "12. `__str__`\n",
    "    - arguments:\n",
    "        - self\n",
    "    - returns:\n",
    "        - a string describing the value and derivatives of the current instance\n",
    "        \n",
    "        \n",
    "13. `__len__`\n",
    "    - arguments:\n",
    "        - self\n",
    "    - returns:\n",
    "        - len(self.val); number of function values stored within the fAD object.\n",
    "        \n",
    "        \n",
    "14. `__repr__`\n",
    "    - arguments:\n",
    "        - self\n",
    "    - returns:\n",
    "        - string describing fAD(self.val,self.der)\n",
    "        \n",
    "        \n",
    "15. `get_val()`\n",
    "    - arguments:\n",
    "        - self\n",
    "    - returns:\n",
    "        - self.val formatted correctly\n",
    "\n",
    "\n",
    "16. `get_jac()`\n",
    "    - arguments:\n",
    "        - self\n",
    "    - returns:\n",
    "        - self.der formatted correctly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class: rAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class encapsulates the fundamental machinery of forward mode automatic differentiation, and is capable of dealing with both single and multi-variable cases.\n",
    "\n",
    "#### Dependencies\n",
    "\n",
    "- numpy (imported as np): used for numerous mathematical (e.g. trigonometric, logarithmic operations)\n",
    "- numbers: used to ensure user passes numerical values.\n",
    "\n",
    "#### Attributes (Data Structures)\n",
    "- val: array of floats (of size 1 or more)\n",
    "    - Numeric values, indicating the value of each entry in the current AutoDiff instance. For cases with only one function, val will size 1. For vector-valued functions, val can be longer.\n",
    "    \n",
    "#### Methods \n",
    "(The following demonstrations are for the case when there is only one value in self.val. When the AD object is in higher dimension, storing the values in an array allows us to simply apply the computation to each entry.)\n",
    "\n",
    "0. `__init__`:\n",
    "    - arguments: \n",
    "        - a list/array of AD instances\n",
    "    - sets self.val as a list of 'val' attributes of the input AD instances\n",
    "    - creates empty list for children and sets derivative value to None\n",
    "\n",
    "\n",
    "1. `grad`:\n",
    "    - arguments:\n",
    "        - self\n",
    "    - returns:\n",
    "        - gradient of outer object with respect to this object; calling this before variable.der / variable.get_der() will update derivatives for outer variable from None to its gradient with respect to this object.\n",
    "\n",
    "2. `__add__` & `__radd__`: \n",
    "    - arguments:\n",
    "        - self\n",
    "        - other: a float, int, or rAD object\n",
    "    - returns: \n",
    "        - if other is a rAD -> a new rAD instance with new.val = self.val + other.val\n",
    "        - if other is a numeric value -> a new AD instance with new.val = self.val + other\n",
    "        - appends to the children of self and other (if other is rAD) a tuple of weight = self.val, and the new rAD object.\n",
    "\n",
    "\n",
    "3. `__sub__`\n",
    "    - arguments:\n",
    "\t\t- self\n",
    "\t\t- other: a float, int, or rAD object\n",
    "\t- returns: \n",
    "\t\t- if other is a rAD -> a new rAD instance with new.val = self.val - other.val\n",
    "\t\t- if other is a numeric value -> a new AD instance with new.val = self.val - other\n",
    "        - appends to the children of self and other (if other is rAD) a tuple of weight = self.val (times 1 and -1 respectively), and the new rAD object. \n",
    "        \n",
    "        \n",
    "4. `__rsub__`\n",
    "    - arguments:\n",
    "\t\t- self\n",
    "\t\t- other: a float, int, or rAD object\n",
    "\t- returns: \n",
    "\t\t- if other is a rAD -> a new rAD instance with new.val= other.val - self.val\n",
    "\t\t- if other is a numeric value -> a new AD instance with new.val = other - self.val\n",
    "        - appends to the children of self and other (if other is rAD) a tuple of weight = self.val (times -1 and 1 respectively), and the new rAD object. \n",
    "\n",
    "\n",
    "\n",
    "5. `__mul__` & `__rmul__`\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t\t- other: a float, int, or rAD\n",
    "\t- returns: \n",
    "\t\t- if other is a rAD -> a new rAD instance with new.val = self.val \\* other.val, \n",
    "        - appends (other.val, new value) to self's children and (self.val, new value) to other's children\n",
    "\t\t- if other is a numeric value -> a new rAD instance with new.val = self.val \\* other, new.der = self.der \\* other\n",
    "        - appends (other\\*self.value, new value) to self's children\n",
    "\n",
    "\n",
    "6. `__truediv__`\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t\t- other: a float, int, or rAD\n",
    "\t- returns: \n",
    "\t\t- if other is a rAD -> a new rAD instance with new.val = self.val / other.val \n",
    "        - appends (1/other.val, new value) to self's children and ((-self.val / other.val\\*\\*2),new value) to other's children \n",
    "\t\t- if other is a numeric value -> a new rAD instance with new.val = self.val / other, and (1/other, new value) is appended to self's children.\n",
    "\t- raises:\n",
    "\t\t- ZeroDivisionError when other.val = 0 or other = 0\n",
    "\n",
    "\n",
    "7. `__rtruediv__`\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t\t- other: a float, int, or rAD\n",
    "\t- returns: \n",
    "\t\t- if other is a rAD -> a new rAD instance with new.val = other.val / self.val, \n",
    "        - appends (-other.val / self.val\\*\\*2, new value) to self's children and (1/self.val, new value) to other's children\n",
    "\t\t- if other is a numeric value -> a new rAD instance and appends (-other/self.val\\*\\*2, new value) to self's children\n",
    "\t- raises:\n",
    "\t\t- ZeroDivisionError when self.val = 0\n",
    "\n",
    "\n",
    "8. `__pow__`\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t\t- exp: a float, int, or rAD\n",
    "\t- returns:\n",
    "\t\t- if exp is a rAD -> a new rAD instance with new.val = self.val \\*\\* exp.val\n",
    "        - appends (self.val\\*\\*(other.val-1)\\*other.val, new value) to self's children and (self.val\\*\\*other.val\\*np.log(self.val), new value) to other's children\n",
    "\t\t- if other is a numeric value -> a new rAD instance with new.val = self.val \\*\\* exp, and appends self's children with (self.val\\*\\*(other-1)\\*other, new value)\n",
    "\n",
    "\n",
    "9. `__rpow__`\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t\t- base: a float, int, or rAD\n",
    "\t- returns:\n",
    "\t\t- if base is a rAD -> a new rAD instance with new.val = base.val\\*\\*self.val\n",
    "        - appends self's children with (other.val\\*\\*self.val\\*np.log(other.val), new value) and other's children is appended with (other.val\\*\\*(self.val-1)\\*self.val, new value)\n",
    "\t\t- if base is a numeric value -> a new rAD instance with new.val = base\\*\\*self.val, and self' children  is appended with (other\\*\\*self.val\\*np.log(other), new value)\n",
    "\n",
    "\n",
    "10. `__neg__`\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t- returns:\n",
    "\t\t- a new AD instance with new.val = -self.val, and (-1,new value) is appended to self's children      \n",
    "\n",
    "11. `__abs__`\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t- returns:\n",
    "\t\t- a new AD instance with new.val = abs(self.val), self's children is appended with (self.val / abs(self.val), new value)\n",
    "\n",
    "\n",
    "\n",
    "12. `__eq__`\n",
    "    - arguments:\n",
    "        - self\n",
    "        - other: a rAD instance\n",
    "    - returns:\n",
    "        - 'True' if self.val==other.val and self.der==other.der, 'False' otherwise\n",
    "        \n",
    "13. `__ne__`\n",
    "    - arguments:\n",
    "        - self\n",
    "        - other: a rAD instance\n",
    "    - returns:\n",
    "        - 'False' if self.val==other.val and self.der==other.der, 'True' otherwise\n",
    "        \n",
    "14. `__str__`\n",
    "    - arguments:\n",
    "        - self\n",
    "    - returns:\n",
    "        - a string describing the value and derivatives of the current instance\n",
    "        \n",
    "15. `get_val()`\n",
    "    - arguments:\n",
    "        - self\n",
    "    - returns:\n",
    "        - self.val formatted correctly\n",
    "\n",
    "\n",
    "16. `get_grad()`\n",
    "    - arguments:\n",
    "        - self\n",
    "    - returns:\n",
    "        - gradient of self with respect to outer object\n",
    "        \n",
    "17. `outer`\n",
    "    - arguments:\n",
    "        - self\n",
    "    - returns:\n",
    "        - nothing\n",
    "        - sets this object to be the outer variable for a function by setting self.der = 1 (derivative of self with respect to self = 1).\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions: create, stack, sin, cos, log, exp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies\n",
    "\n",
    "- numpy (imported as np): used for numerous mathematical (e.g. trigonometric operations)\n",
    "- numbers: used to ensure user passes numerical values.\n",
    "- math: used for logarithms with variable bases.\n",
    "\n",
    "The following functions exist outside the AutoDiff class:\n",
    "\n",
    "1. `create_f`\n",
    "    - allows the users to quickly create multiple fAD instances\n",
    "    - arguments:\n",
    "        - val: a list of values \n",
    "        - der: optional, assigned derivative values\n",
    "    - returns:\n",
    "        - a list of fAD objects\n",
    "\n",
    "1. `create_r`\n",
    "    - allows the users to quickly create multiple fAD instances\n",
    "    - arguments:\n",
    "        - val: a list of values \n",
    "    - returns:\n",
    "        - a list of rAD objects\n",
    "\n",
    "2. `stack_f`\n",
    "    - allows users to stack multiple fAD instances into one high-dimentional fAD instance\n",
    "    - arguments:\n",
    "        - vals: a list of multiple fAD instances\n",
    "    - returns:\n",
    "        - one fAD object, with *val* = an array of *val*'s of the fAD instances in the argument fADs, and *der* = an array of *der*'s of the fAD instances in the argument\n",
    "        \n",
    "3. `stack_r`\n",
    "    - allows users to stack multiple rAD instances into one high-dimentional rAD instance\n",
    "    - arguments:\n",
    "        - vals: a list of multiple rAD instances\n",
    "        - functions: list of multiple functions to be combined\n",
    "    - returns:\n",
    "        - jacobian of functions\n",
    "        \n",
    "3. `sin`\n",
    "\t- arguments:\n",
    "\t\t- x (fAD, rAD or numeric)\n",
    "\t- returns:\n",
    "\t\t- returns sin(x) as the appropriate object (fAD, rAD or numeric) with correct derivative/children as appropriate.\n",
    "\n",
    "3. `cos`\n",
    "\t- arguments:\n",
    "\t\t- x (fAD, rAD or numeric)\n",
    "\t- returns:\n",
    "\t\t- returns cos(x) as the appropriate object (fAD, rAD or numeric) with correct derivative/children as appropriate.\n",
    "\n",
    "\n",
    "3. `tan`\n",
    "\t- arguments:\n",
    "\t\t- x (fAD, rAD or numeric)\n",
    "\t- returns:\n",
    "\t\t- returns tan(x) as the appropriate object (fAD, rAD or numeric) with correct derivative/children as appropriate.\n",
    "        \n",
    "3. `arcsin`\n",
    "\t- arguments:\n",
    "\t\t- x (fAD, rAD or numeric)\n",
    "\t- returns:\n",
    "\t\t- returns arcsin(x) as the appropriate object (fAD, rAD or numeric) with correct derivative/children as appropriate.\n",
    "\n",
    "3. `arccos`\n",
    "\t- arguments:\n",
    "\t\t- x (fAD, rAD or numeric)\n",
    "\t- returns:\n",
    "\t\t- returns arccos(x) as the appropriate object (fAD, rAD or numeric) with correct derivative/children as appropriate.\n",
    "        \n",
    "3. `arctan`\n",
    "\t- arguments:\n",
    "\t\t- x (fAD, rAD or numeric)\n",
    "\t- returns:\n",
    "\t\t- returns arctan(x) as the appropriate object (fAD, rAD or numeric) with correct derivative/children as appropriate. \n",
    "        \n",
    "3. `sinh`\n",
    "\t- arguments:\n",
    "\t\t- x (fAD, rAD or numeric)\n",
    "\t- returns:\n",
    "\t\t- returns sinh(x) as the appropriate object (fAD, rAD or numeric) with correct derivative/children as appropriate.\n",
    "        \n",
    "3. `cosh`\n",
    "\t- arguments:\n",
    "\t\t- x (fAD, rAD or numeric)\n",
    "\t- returns:\n",
    "\t\t- returns cosh(x) as the appropriate object (fAD, rAD or numeric) with correct derivative/children as appropriate.\n",
    "        \n",
    "3. `tanh`\n",
    "\t- arguments:\n",
    "\t\t- x (fAD, rAD or numeric)\n",
    "\t- returns:\n",
    "\t\t- returns tanh(x) as the appropriate object (fAD, rAD or numeric) with correct derivative/children as appropriate.\n",
    "\n",
    "5. `exp`\n",
    "\t- arguments:\n",
    "\t\t- x (fAD, rAD or numeric)\n",
    "\t- returns:\n",
    "\t\t- returns exp(x) as the appropriate object (fAD, rAD or numeric) with correct derivative/children as appropriate.\n",
    "\n",
    "6. `log`\n",
    "\t- arguments:\n",
    "\t\t- x (AutoDiff or numeric)\n",
    "\t- returns:\n",
    "\t\t- returns log(x) as the appropriate object (fAD, rAD or numeric) with correct derivative/children as appropriate.\n",
    "        \n",
    "5. `sqrt`\n",
    "\t- arguments:\n",
    "\t\t- x (fAD, rAD or numeric)\n",
    "\t- returns:\n",
    "\t\t- returns sqrt(x) as the appropriate object (fAD, rAD or numeric) with correct derivative/children as appropriate.\n",
    "        \n",
    "5. `logistic`\n",
    "    - arguments:\n",
    "\t\t- x (fAD, rAD or numeric)\n",
    "\t- returns:\n",
    "\t\t- returns logistic(x) as the appropriate object (fAD, rAD or numeric) with correct derivative/children as appropriate.\n",
    "        \n",
    "5. `mul_by_row`\n",
    "\t- arguments:\n",
    "\t\t- val: array of values\n",
    "        - der: array of derivatives\n",
    "\t- returns:\n",
    "\t\t- performs row-wise multiplication for forward-mode autodiff objects, facilitating calculations with 2-dimensional derivatives.\n",
    "        \n",
    "5. `reset_der`\n",
    "\t- arguments:\n",
    "\t\t- rADs: single instance or array of rAD objects\n",
    "\t- returns:\n",
    "\t\t- nothing\n",
    "        - resets children and derivative values for all rAD objects given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _d) Future_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main improvement that could be made is to improve the user-friendliness and intuitiveness of the reverse mode. Due to the fact that the values and computational tree are created in one direction and the derivatives are determined by traversing the other direction, the usage is slightly counter-intuitive; the derivative of y with respect to x is an attribute of x, not y. An ideal usage might be that the derivative of y with respect to x is an attribute of x could be attained through calling something like y.grad(x), but to do this the objects x and y must be connect in some way, and it is not clear how this would be achieved without decreasing user-friendliness further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
