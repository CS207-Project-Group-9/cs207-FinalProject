{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The software implements ‘Automatic Differentiation’ (AD). This is a technique to computationally evaluate the derivative of a specified function. Importantly, AD is not the same as symbolic differentiation or numerical differentiation, and holds important advantages over both. Symbolic differentiation, which is equivalent to analytically solving differential equations, can find the exact solution (with machine precision), but is very computationally expensive, and so with very large functions can be infeasible. Numerical differentiation, which uses the finite-difference approximation method, is computationally efficient, but is ultimately only approximate, and can be subject to both rounding error and discretisation error, meaning that it cannot be perfectly accurate. Both of these ‘traditional’ methods of differentiation run into problems and inefficiencies when calculating higher derivatives and partial derivatives with respect to many inputs (which is an important component of gradient-based optimisation methods). \n",
    "Automatic differentiation solves all these problems as it is able to solve derivatives to machine precision with comparative computational efficiency. As a result, automatic differentiation has incredibly important applications; in its ‘reverse-mode’ (discussed below), it is the basis of back-propagation, a fundamental process in neural network machine learning algorithms - as such this technique is leveraged by open-source machine learning libraries such as TensorFlow. A result of its efficient accuracy and iterative method, AD is capable of algorithmic differentiation: Because of the fact that every computer program, from mathematical algorithms to web-pages, can be expressed as a sequence and combination of arithmetic operations and elementary functions, the derivative of any computer program can be found using automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic differentiation is essentially the iterative application of the chain-rule. As mentioned above, any function can be considered a sequence of basic arithmetic operations or elementary functions (+,-,x,/,sin,cos,exp etc.) and so any function can be interpreted in the following way (albeit often less simply):\n",
    "\t$$y = f(x) = f(g(h((x)))$$\n",
    "\n",
    "This can be rewritten as:\n",
    "$$y = f(g(h(x0) = f(g(x1) = f(x2)) = x3$$\n",
    "\t\n",
    "Often, this decomposition is represented as an acyclic, directed computational graph that illustrates the route from the base function x0 to y.\n",
    "In forward mode, automatic differentiation works by decomposing the function into this structure, and working through each component function finding the derivative using the chain rule ‘inside out’. That is to say, dx0/dx is found first, following by dx1/dx and so on until dy/dx itself is found. All this requires initial values to be set for x0, and x0’.\n",
    "\tReverse mode, however, works in the opposite direction; rather than finding the derivative of the most fundamental component, and then finding the derivative of parent expressions in terms of these children components recursively until the final gradient is found, reverse mode goes the other way. It finds the derivative of each ‘child’ function in terms of its parent function recursively until the basic level derivative is found, at which point the final gradient can be found.\n",
    "\tOne way of achieving forward mode AD is to use dual numbers. These are an extension of real numbers, somewhat analogous to imaginary numbers, such that every number additionally contains a dual component, $\\epsilon$, where $\\epsilon^2$ = 0. Given any polynomial function (or, in fact, any analytic real function via its Taylor series), if we replace x with (x+x'$\\epsilon$), we find that the function will become: f(x) + f'(x)$\\epsilon$. This provides a routine to automatically compute the derivative of the function f(x), and so is used in forward AD.\n",
    "\n",
    "Sources: https://en.wikipedia.org/wiki/Automatic_differentiation\n",
    "\t   http://www.columbia.edu/~ahd2125/post/2015/12/5/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. How to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current AutoDiff package implements forward-mode automatic differentiation. Users should import the package using the following commands:\n",
    "\n",
    "```python\n",
    ">>> import AutoDiff \n",
    ">>> #or\n",
    ">>> from AutoDiff import AD, AD_create, AD_stack\n",
    "```\n",
    "\n",
    "Importation of individual methods is not necessary but possible. While the current package employs instances and methods of Numpy and Math, importation of these modules is unnecessary.  \n",
    "\n",
    "To instantiate an automatic differentiation [AD] object, users should define variables and values before defining function(s) for differentiation. There is no limit on the number of variables a function can include for AD. By wrapping a variable in AD_create( ), users will have instantiated an AD object: \n",
    "\n",
    "```python\n",
    ">>> #example\n",
    ">>> a = 5.0\n",
    ">>> x = AD(a) #returns a as an AD object stored as x\n",
    ">>> print(x)\n",
    "    <__main__.AD object at 0x103bi28d7> \n",
    ">>> print(x.val)\n",
    "    5.0\n",
    "```\n",
    "\n",
    "Alternatively, users may initiate multiple AD objects by passing an array/list of variables. For example:\n",
    "\n",
    "```python\n",
    ">>> x, y, z = 1.0, 3.0, 4.0 #assign values to multiple variables\n",
    ">>> vars = [x, y, z] #initiate a list of variables\n",
    ">>> varsAD = AD_create(vars)\n",
    ">>> print(varsAD)\n",
    "    [<__main__.AD object at 0x103bi28d7>, <__main__.AD object at 0x124bi28c5>,<__main__.AD object at 3x156bw22c0>]\n",
    ">>> print(varsAD[1].val)\n",
    "    3.0\n",
    "```\n",
    "\n",
    "Users should then define function(s) for AD, function value and derivative will be returned by calling \".val\" and \".der\" respectively. \n",
    "\n",
    "```python\n",
    ">>> #example\n",
    ">>> x, y, z = 1.0, 3.0, 4.0\n",
    ">>> vars = [x, y, z]\n",
    ">>> varsAD = AD_create(vars)\n",
    ">>> function = 2x + 5y**2 + z\n",
    ">>> print(function.val, function.der)\n",
    "    53.0, [2.0, 30.0, 1.0] #return partial derivatives in the order of df/dx, df/dy, df/dz\n",
    "```\n",
    "\n",
    "In the case where output function is multidimentional, users can define functions and compute values and derivatives as such:\n",
    "\n",
    "```python\n",
    ">>> #example\n",
    ">>> x, y, z = 1.0, 3.0, 4.0\n",
    ">>> vars = [x, y, z]\n",
    ">>> varsAD = AD_create(vars)\n",
    ">>> f1 = x + 2y + 3z\n",
    ">>> f2 = 2x + y + z\n",
    ">>> f = AD_stack([f1, f2]) #return f as an AD object containing two AD objects\n",
    ">>> print(f.val, f.der)\n",
    ">>> [19.0, 9.0] [[1, 2, 3], [2, 1, 1]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Software Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Directory Structure \n",
    "\n",
    "The following is our envisioned directory structure:\n",
    "\n",
    "```\n",
    "CS207-FinalProject\\\n",
    "                   AutoDiff\\\n",
    "                            AutoDiff/\n",
    "                                    __init__.py\n",
    "                                    AD.py\n",
    "                            Tests/\n",
    "                                 __init__.py\n",
    "                                 AD_Test.py\n",
    "                                 setup.cfg\n",
    "                                 .travis.yml  \n",
    "                            README.md\n",
    "                            setup.py\n",
    "                            LICENSE\n",
    "```\n",
    "\n",
    "Speaking from the current state of project, the AutoDiff package contains one module \"AD.py,\" which computes and outputs function value, derivative, and partial derivatives using forward-mode automatic differentiation. This will be the module that users import for automatic differentiation. \n",
    "\n",
    "The Tests subdirectory currently contains one module. It includes combinations of tests following pytest to ensure that methods from the AD module are executed correctly, and that proper errors are returned when user inputs are incorrect.   \n",
    "\n",
    "We plan to implement reverse-mode automatic differentiation upon completing the development of forward-mode automatic differentiation. The implementation will fall under a separate module, \"AD_r.py,\" which will be added to the AutoDiff package. A separate test module, \"AD_r_Test.py\" will be added to the Tests subdirectory.   \n",
    "\n",
    "* Test Suite and Package Distribution\n",
    "\n",
    "We will use Travis CI and Coveralls to host our test suite. \n",
    "Travis CI: https://travis-ci.org/CS207-Project-Group-9/cs207-FinalProject.svg?branch=master\n",
    "Coveralls: https://coveralls.io/repos/github/CS207-Project-Group-9/cs207-FinalProject/badge.svg?branch=master\n",
    "\n",
    "We will deploy our package using PyPI. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 1: AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describes variables in 1-Dimension\n",
    "\n",
    "#### Attributes\n",
    "- val: float\n",
    "    - A numeric value, indicating the value of the current AD instance.\n",
    "    \n",
    "- der: array of float\n",
    "    - Suppose there are m elementary variables, 'der' would be an array of m elements, each representing the derivative value with respect to a certain elementary variable.\n",
    "    - We store these values in an array so that when our later computation is easier.\n",
    "\n",
    "#### Methods\n",
    "0. \\__init__:\n",
    "    - arguments: \n",
    "        - val: float, representing the value\n",
    "        - der: a list of floats, representing the derivatives with respect to elementary variables; optional, default=1\n",
    "    - sets self.val=val, self.der=der\n",
    "\n",
    "\n",
    "1. \\__add__ & \\__radd__: \n",
    "    - arguments:\n",
    "        - self\n",
    "        - other: a float, int, or AD\n",
    "    - returns: \n",
    "        - if other is an AD -> a new AD instance with new.val = self.val + other.val, new.der = self.der + other.der\n",
    "        - if other is a numeric value -> a new AD instance with new.val = self.val + other, new.der = self.der\n",
    "\n",
    "\n",
    "2. \\__sub__ & \\__rsub__\n",
    "    - arguments:\n",
    "\t\t- self\n",
    "\t\t- other: a float, int, or AD\n",
    "\t- returns: \n",
    "\t\t- if other is an AD -> a new AD instance with new.val = self.val - other.val, new.der = self.der - other.der\n",
    "\t\t- if other is a numeric value -> a new AD instance with new.val = self.val - other, new.der = self.der\n",
    "\n",
    "\n",
    "3. \\__mul__ & \\__rmul__\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t\t- other: a float, int, or AD\n",
    "\t- returns: \n",
    "\t\t- if other is an AD -> a new AD instance with new.val = self.val \\* other.val, new.der = self.val \\* other.der + self.der \\* other.val\n",
    "\t\t- if other is a numeric value -> a new AD instance with new.val = self.val \\* other, new.der = self.der \\* other\n",
    "\n",
    "\n",
    "4. \\__div__ & \\__rdiv__\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t\t- other: a float, int, or AD\n",
    "\t- returns: \n",
    "\t\t- if other is an AD -> a new AD instance with new.val = self.val / other.val, new.der = (self.der \\* other.val + self.val \\* other.der)/(other.val\\*\\*2)\n",
    "\t\t- if other is a numeric value -> a new AD instance with new.val = self.val / other, new.der = self.der / other\n",
    "\t- raises:\n",
    "\t\t- ZeroDivisionError when other.val = 0\n",
    "\n",
    "\n",
    "5. \\__pow__\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t\t- k: a float, int, or AD\n",
    "\t- returns:\n",
    "\t\t- if other is an AD -> a new AD instance with new.val = self.val \\*\\* other.val, new.der = other.val \\* self.val \\*\\* (other.val - 1) \\* self.der + log(self.val) \\* self.val \\*\\* other.val \\* other.der\n",
    "\t\t- if other is a numeric value -> a new AD instance with new.val = self.val \\*\\* k, new.der = k \\* (self.val\\*\\*(k-1)) \\* self.der\n",
    "\n",
    "\n",
    "6. \\__abs__\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t- returns:\n",
    "\t\t- a new AD instance with new.val = abs(self.val), new.der = sign(self.val) \\* self.der\n",
    "\n",
    "\n",
    "7. sin\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t- returns:\n",
    "\t\t- a new AD instance with new.val = sin(self.val), new.der = cos(self.val) \\* self.der\n",
    "\n",
    "\n",
    "8. cos\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t- returns:\n",
    "\t\t- a new AD instance with new.val = cos(self.val), new.der = -sin(self.val) \\* self.der\n",
    "\n",
    "\n",
    "9. tan\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t- returns:\n",
    "\t\t- a new AD instance with new.val = cos(self.val), new.der = 1/(cos(self.val) \\*\\* 2) \\* self.der\n",
    "\n",
    "\n",
    "10. exp\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t- returns:\n",
    "\t\t- a new AD instance with new.val = exp(self.val), new.der = exp(self.val) \\* self.der\n",
    "\n",
    "\n",
    "11. log\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t- returns:\n",
    "\t\t- a new AD instance with new.val = log(self.val), new.der = self.der/self.val\n",
    "\t- raises: (????)\n",
    "\t\texception when self.val <= 0 \n",
    "\n",
    "12. get_val()\n",
    "    - arguments:\n",
    "        - self\n",
    "    - returns:\n",
    "        - self.val\n",
    "\n",
    "13. get_der(i=1)\n",
    "    - arguments:\n",
    "        - self\n",
    "    - returns:\n",
    "        - self.der\n",
    "\n",
    "14. \\__eq__\n",
    "    - arguments:\n",
    "        - self\n",
    "        - other: an AD instance\n",
    "    - returns:\n",
    "        - 'True' if self.val==other.val and self.der==other.der, 'False' otherwise\n",
    "        \n",
    "15. \\__str__\n",
    "    - arguments:\n",
    "        - self\n",
    "    - returns:\n",
    "        - a string describing the value and derivatives of the current instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 2: AD_System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describes variables in n-Dimension\n",
    "\n",
    "#### Attributes\n",
    "- val: array of float\n",
    "    - Numeric values, indicating the value of each entry in the current AD_System instance.\n",
    "    \n",
    "- der: 2D array of float\n",
    "    - Suppose there are m elementary variables and n variables in the AD_System, 'der' would be a n\\*m array of elements, with the (i,j) entry representing the derivative value of the i-th AD with respect to the j-th elementary variable.\n",
    "\n",
    "#### Methods\n",
    "0. \\__init__:\n",
    "    - arguments: \n",
    "        - a list/array of AD instances\n",
    "    - sets self.val as a list of 'val' attributes of the input AD instances\n",
    "    - combines the 'der' attributes of the input AD instances as a 2D array and save as self.der\n",
    "\n",
    "Class AD_System will also implement the following functions, very similar to the simple AD class:\n",
    "1. \\__add__ & \\__radd__\n",
    "2. \\__sub__ & \\__rsub__\n",
    "3. \\__mul__ & \\__rmul__\n",
    "4. \\__div__ & \\__rdiv__\n",
    "5. \\__pow__\n",
    "6. \\__abs__\n",
    "7. sin\n",
    "8. cos\n",
    "9. tan\n",
    "10. exp\n",
    "11. log\n",
    "12. get_val()\n",
    "13. get_der(i=1)\n",
    "14. \\__eq__\n",
    "15. \\__str__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need **numpy** for impletation and **pytest** and **doctest** for testing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
