{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS207 Project Group 9\n",
    "# Milestone 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The software implements ‘Automatic Differentiation’ (AD). This is a technique to computationally evaluate the derivative of a specified function. Importantly, AD is not the same as symbolic differentiation or numerical differentiation, and holds important advantages over both. Symbolic differentiation, which is equivalent to analytically solving differential equations, can find the exact solution (with machine precision), but is very computationally expensive, and so with very large functions can be infeasible. Numerical differentiation, which uses the finite-difference approximation method, is computationally efficient, but is ultimately only approximate, and can be subject to both rounding error and discretisation error, meaning that it cannot be perfectly accurate. Both of these ‘traditional’ methods of differentiation run into problems and inefficiencies when calculating higher derivatives and partial derivatives with respect to many inputs (which is an important component of gradient-based optimisation methods). \n",
    "\n",
    "\n",
    "Automatic differentiation solves all these problems as it is able to solve derivatives to machine precision with comparative computational efficiency. As a result, automatic differentiation has incredibly important applications; in its ‘reverse-mode’ (discussed below), it is the basis of back-propagation, a fundamental process in neural network machine learning algorithms - as such this technique is leveraged by open-source machine learning libraries such as TensorFlow. A result of its efficient accuracy and iterative method, AD is capable of algorithmic differentiation: Because of the fact that every computer program, from mathematical algorithms to web-pages, can be expressed as a sequence and combination of arithmetic operations and elementary functions, the derivative of any computer program can be found using automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic differentiation is essentially the iterative application of the chain-rule. As mentioned above, any function can be considered a sequence of basic arithmetic operations or elementary functions (addition, multiplication, division, subtraction, trigonometric functions, exponential, logarithms etc.) and so any function can be interpreted in the following way (albeit often less simply):\n",
    "\t$$y = f(x) = f(g(h((x)))$$\n",
    "\n",
    "This can be rewritten as:\n",
    "$$y = f(g(h(x0))) = f(g(x1)) = f(x2) = x3$$\n",
    "\t\n",
    "Often, this decomposition is represented as an acyclic, directed computational graph that illustrates the route from the base function x0 to y, as illustrated by the example below:\n",
    "\n",
    "$ x_0\\rightarrow^{h(x)}x_1\\rightarrow^{g(x)}x_2\\rightarrow^{f(x)}x_3\\rightarrow y $\n",
    "\n",
    "\n",
    "\n",
    "In forward mode, automatic differentiation works by decomposing the function into this structure, and working through each component function finding the derivative using the chain rule ‘inside out’. That is to say, dx0/dx is found first, following by dx1/dx and so on until dy/dx itself is found. All this requires initial values to be set for x0, and x0’.\n",
    "\n",
    "\n",
    "Reverse mode, however, works in the opposite direction; rather than finding the derivative of the most fundamental component, and then finding the derivative of parent expressions in terms of these children components recursively until the final gradient is found, reverse mode goes the other way. It finds the derivative of each ‘child’ function in terms of its parent function recursively until the basic level derivative is found, at which point the final gradient can be found.\n",
    "\n",
    "\n",
    "One way of achieving forward mode AD is to use dual numbers. These are an extension of real numbers, somewhat analogous to imaginary numbers, such that every number additionally contains a dual component, $\\epsilon$, where $\\epsilon^2$ = 0. Given any polynomial function (or, in fact, any analytic real function via its Taylor series), if we replace x with (x+x'$\\epsilon$), we find that the function will become: f(x) + f'(x)$\\epsilon$. This provides a routine to automatically compute the derivative of the function f(x), and so is used in forward AD.\n",
    "\n",
    "Sources: https://en.wikipedia.org/wiki/Automatic_differentiation,\n",
    "\t   http://www.columbia.edu/~ahd2125/post/2015/12/5/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. How to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current AutoDiff package implements forward-mode automatic differentiation. Users should import the package using the following commands:\n",
    "\n",
    "```python\n",
    ">>> import AutoDiff \n",
    ">>> #or\n",
    ">>> from AutoDiff import AD, AD_create, AD_stack\n",
    "```\n",
    "\n",
    "Importation of individual methods is not necessary but possible. While the current package employs instances and methods of Numpy and Math, importation of these modules is unnecessary.  \n",
    "\n",
    "To instantiate an automatic differentiation [AD] object, users should define variables and values before defining function(s) for differentiation. There is no limit on the number of variables a function can include for AD. By wrapping a variable in AD_create( ), users will have instantiated an AD object: \n",
    "\n",
    "```python\n",
    ">>> #example\n",
    ">>> a = 5.0\n",
    ">>> x = AD(a) #returns a as an AD object stored as x\n",
    ">>> print(x)\n",
    "    <__main__.AD object at 0x103bi28d7> \n",
    ">>> print(x.val)\n",
    "    5.0\n",
    "```\n",
    "\n",
    "Alternatively, users may initiate multiple AD objects by passing an array/list of variables. For example:\n",
    "\n",
    "```python\n",
    ">>> x, y, z = 1.0, 3.0, 4.0 #assign values to multiple variables\n",
    ">>> vars = [x, y, z] #initiate a list of variables\n",
    ">>> varsAD = AD_create(vars)\n",
    ">>> print(varsAD)\n",
    "    [<__main__.AD object at 0x103bi28d7>, <__main__.AD object at 0x124bi28c5>,<__main__.AD object at 3x156bw22c0>]\n",
    ">>> print(varsAD[1].val)\n",
    "    3.0\n",
    "```\n",
    "\n",
    "Users should then define function(s) for AD, function value and derivative will be returned by calling \".val\" and \".der\" respectively. \n",
    "\n",
    "```python\n",
    ">>> #example\n",
    ">>> x, y, z = 1.0, 3.0, 4.0\n",
    ">>> vars = [x, y, z]\n",
    ">>> varsAD = AD_create(vars)\n",
    ">>> function = 2x + 5y**2 + z\n",
    ">>> print(function.val, function.der)\n",
    "    53.0, [2.0, 30.0, 1.0] #return partial derivatives in the order of df/dx, df/dy, df/dz\n",
    "```\n",
    "\n",
    "In the case where output function is multidimentional, users can define functions and compute values and derivatives as such:\n",
    "\n",
    "```python\n",
    ">>> #example\n",
    ">>> x, y, z = 1.0, 3.0, 4.0\n",
    ">>> vars = [x, y, z]\n",
    ">>> varsAD = AD_create(vars)\n",
    ">>> f1 = x + 2y + 3z\n",
    ">>> f2 = 2x + y + z\n",
    ">>> f = AD_stack([f1, f2]) #return f as an AD object containing two AD objects\n",
    ">>> print(f.val, f.der)\n",
    ">>> [19.0, 9.0] [[1, 2, 3], [2, 1, 1]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Software Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Directory Structure \n",
    "\n",
    "The following is our envisioned directory structure:\n",
    "\n",
    "```\n",
    "CS207-FinalProject\\\n",
    "                   AutoDiff\\\n",
    "                            AutoDiff/\n",
    "                                    __init__.py\n",
    "                                    AD.py\n",
    "                            Tests/\n",
    "                                 __init__.py\n",
    "                                 AD_Test.py\n",
    "                                 setup.cfg\n",
    "                                 .travis.yml  \n",
    "                            README.md\n",
    "                            setup.py\n",
    "                            LICENSE\n",
    "```\n",
    "\n",
    "Speaking from the current state of project, the AutoDiff package contains one module \"AD.py,\" which computes and outputs function value, derivative, and partial derivatives using forward-mode automatic differentiation. This will be the module that users import for automatic differentiation. \n",
    "\n",
    "The Tests subdirectory currently contains one module. It includes combinations of tests following pytest to ensure that methods from the AD module are executed correctly, and that proper errors are returned when user inputs are incorrect.   \n",
    "\n",
    "We plan to implement reverse-mode automatic differentiation upon completing the development of forward-mode automatic differentiation. The implementation will fall under a separate module, \"AD_r.py,\" which will be added to the AutoDiff package. A separate test module, \"AD_r_Test.py\" will be added to the Tests subdirectory.   \n",
    "\n",
    "* Test Suite and Package Distribution\n",
    "\n",
    "We will use Travis CI and Coveralls to host our test suite. \n",
    "Travis CI: https://travis-ci.org/CS207-Project-Group-9/cs207-FinalProject.svg?branch=master\n",
    "Coveralls: https://coveralls.io/repos/github/CS207-Project-Group-9/cs207-FinalProject/badge.svg?branch=master\n",
    "\n",
    "We will deploy our package using PyPI. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 1: AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plan to develop only one class for the forward mode auto-differentiation, called AD, that will allow operations of high-dimensional functions.\n",
    "\n",
    "#### Attributes (Data Structure)\n",
    "- val: array of float\n",
    "    - Numeric values, indicating the value of each entry in the current AD_System instance.\n",
    "    \n",
    "- der: 2D array of float\n",
    "    - Suppose there are m elementary variables and n variables in the AD_System, 'der' would be a n\\*m array of elements, with the (i,j) entry representing the derivative value of the i-th AD with respect to the j-th elementary variable.\n",
    "\n",
    "#### Methods \n",
    "(The following demonstrations are for the case when there is only one value in self.val. When the AD object is in higher dimension, storing the values in an array allows us to simply apply the computation to each entry.)\n",
    "\n",
    "0. `__init__`:\n",
    "    - arguments: \n",
    "        - a list/array of AD instances\n",
    "    - sets self.val as a list of 'val' attributes of the input AD instances\n",
    "    - combines the 'der' attributes of the input AD instances as a 2D array and save as self.der\n",
    "\n",
    "\n",
    "1. `__add__` & `__radd__`: \n",
    "    - arguments:\n",
    "        - self\n",
    "        - other: a float, int, or AD\n",
    "    - returns: \n",
    "        - if other is an AD -> a new AD instance with new.val = self.val + other.val, new.der = self.der + other.der\n",
    "        - if other is a numeric value -> a new AD instance with new.val = self.val + other, new.der = self.der\n",
    "\n",
    "\n",
    "2. `__sub__` & `__rsub__`\n",
    "    - arguments:\n",
    "\t\t- self\n",
    "\t\t- other: a float, int, or AD\n",
    "\t- returns: \n",
    "\t\t- if other is an AD -> a new AD instance with new.val = self.val - other.val, new.der = self.der - other.der\n",
    "\t\t- if other is a numeric value -> a new AD instance with new.val = self.val - other, new.der = self.der\n",
    "\n",
    "\n",
    "3. `__mul__` & `__rmul__`\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t\t- other: a float, int, or AD\n",
    "\t- returns: \n",
    "\t\t- if other is an AD -> a new AD instance with new.val = self.val \\* other.val, new.der = self.val \\* other.der + self.der \\* other.val\n",
    "\t\t- if other is a numeric value -> a new AD instance with new.val = self.val \\* other, new.der = self.der \\* other\n",
    "\n",
    "\n",
    "4. `__div__` & `__rdiv__`\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t\t- other: a float, int, or AD\n",
    "\t- returns: \n",
    "\t\t- if other is an AD -> a new AD instance with new.val = self.val / other.val, new.der = (self.der \\* other.val + self.val \\* other.der)/(other.val\\*\\*2)\n",
    "\t\t- if other is a numeric value -> a new AD instance with new.val = self.val / other, new.der = self.der / other\n",
    "\t- raises:\n",
    "\t\t- ZeroDivisionError when other.val = 0\n",
    "\n",
    "\n",
    "5. `__pow__`\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t\t- k: a float, int, or AD\n",
    "\t- returns:\n",
    "\t\t- if other is an AD -> a new AD instance with new.val = self.val \\*\\* other.val, new.der = other.val \\* self.val \\*\\* (other.val - 1) \\* self.der + log(self.val) \\* self.val \\*\\* other.val \\* other.der\n",
    "\t\t- if other is a numeric value -> a new AD instance with new.val = self.val \\*\\* k, new.der = k \\* (self.val\\*\\*(k-1)) \\* self.der\n",
    "\n",
    "\n",
    "6. `__abs__`\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t- returns:\n",
    "\t\t- a new AD instance with new.val = abs(self.val), new.der = sign(self.val) \\* self.der\n",
    "\n",
    "\n",
    "7. `sin`\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t- returns:\n",
    "\t\t- a new AD instance with new.val = sin(self.val), new.der = cos(self.val) \\* self.der\n",
    "\n",
    "\n",
    "8. `cos`\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t- returns:\n",
    "\t\t- a new AD instance with new.val = cos(self.val), new.der = -sin(self.val) \\* self.der\n",
    "\n",
    "\n",
    "9. `tan`\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t- returns:\n",
    "\t\t- a new AD instance with new.val = cos(self.val), new.der = 1/(cos(self.val) \\*\\* 2) \\* self.der\n",
    "\n",
    "\n",
    "10. `exp`\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t- returns:\n",
    "\t\t- a new AD instance with new.val = exp(self.val), new.der = exp(self.val) \\* self.der\n",
    "\n",
    "\n",
    "11. `log`\n",
    "\t- arguments:\n",
    "\t\t- self\n",
    "\t- returns:\n",
    "\t\t- a new AD instance with new.val = log(self.val), new.der = self.der/self.val\n",
    "\t- raises:\n",
    "\t\t- exception when self.val <= 0 \n",
    "        \n",
    "\n",
    "12. `get_val()`\n",
    "    - arguments:\n",
    "        - self\n",
    "    - returns:\n",
    "        - self.val\n",
    "\n",
    "\n",
    "13. `get_der(i=1)`\n",
    "    - arguments:\n",
    "        - self\n",
    "    - returns:\n",
    "        - self.der\\[i-1\\]\n",
    "\n",
    "\n",
    "14. `__eq__`\n",
    "    - arguments:\n",
    "        - self\n",
    "        - other: an AD instance\n",
    "    - returns:\n",
    "        - 'True' if self.val==other.val and self.der==other.der, 'False' otherwise\n",
    "        \n",
    "        \n",
    "15. `__str__`\n",
    "    - arguments:\n",
    "        - self\n",
    "    - returns:\n",
    "        - a string describing the value and derivatives of the current instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 2: AD_r\n",
    "\n",
    "\n",
    "For additional expectation, we plan to have another class called AD_r.\n",
    "\n",
    "## Class 3: AD_operators\n",
    "\n",
    "As we proceed in the project, we may have a third class that includes all the operator function that would applied to both AD and AD_r.\n",
    "\n",
    "\n",
    "## Other functions\n",
    "\n",
    "The following functions exist outside the AD class:\n",
    "\n",
    "\n",
    "1. `AD_create`\n",
    "    - allows the users to quickly create multiple AD instances\n",
    "    - arguments:\n",
    "        - val: a list of values \n",
    "        - der: optional, assigned derivative values\n",
    "    - returns:\n",
    "        - a list of AD objects\n",
    "\n",
    "\n",
    "2. `AD_stack`\n",
    "    - allows users to stack multiple AD instances into one high-dimentional AD instance\n",
    "    - arguments:\n",
    "        - ADs: a list of multiple AD instances\n",
    "    - returns:\n",
    "        - one AD object, with *val* = an array of *val*'s of the AD instances in the argument ADs, and *der* = an array of *der*'s of the AD instances in the argument\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need **numpy** for impletation and **pytest** and **doctest** for testing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
